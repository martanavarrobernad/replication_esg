import mne
import numpy as np
from pathlib import Path
import gc
import os

# =============================================================================
# STEP 1: ROBUST BLOCK CLEANING (PROTECTING ERB & AXILLA)
# =============================================================================
BASE_DIR = Path("/home/marta.navarrobernad/Desktop/sub-P02/eeg")
INPUT_FILE = BASE_DIR / "All_Runs_Combined_RefTH6_BL800_COMPLEMENTARY-epo.fif"
TEMP_DIR = BASE_DIR / "TEMP_CHUNKS"
OUT_DIR = BASE_DIR / "CLEAN_EPOCHS_FINAL"

TEMP_DIR.mkdir(exist_ok=True, parents=True)
OUT_DIR.mkdir(exist_ok=True, parents=True)

# Channel definitions
CH_ESG_BASE = ['Iz', 'SC1', 'SC6', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 
               'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19']
CH_ERB = 'ENG_ERB'
CH_AXI = 'ENG_AXILLA'

def get_thr(epochs, ch_type, minimum):
    """Calculates adaptive threshold based on the 98th percentile."""
    try:
        data = epochs.get_data(picks=ch_type)
        if data.size == 0: return minimum
        p2ps = (np.max(data, axis=2) - np.min(data, axis=2)).max(axis=1) * 1e6
        return max(np.percentile(p2ps, 98), minimum)
    except:
        return minimum

def survival_cleaning_robust():
    if not INPUT_FILE.exists():
        print(f"[-] Input file not found: {INPUT_FILE}")
        return

    # Load header to count epochs
    epochs_raw = mne.read_epochs(INPUT_FILE, preload=False, verbose=False)
    total_epochs = len(epochs_raw)
    chunk_size = 400 
    chunk_files = []

    print(f"[+] Starting cleaning process ({total_epochs} total epochs)...")

    for i, start in enumerate(range(0, total_epochs, chunk_size)):
        stop = min(start + chunk_size, total_epochs)
        print(f" >> Processing Block {i+1}: Epochs {start} to {stop}...")
        
        # Load and filter
        chunk = epochs_raw[start:stop].load_data()
        chunk.filter(30, 400, method='iir', 
                     iir_params=dict(order=4, ftype='butter'), 
                     phase='zero', verbose=False)

        # Robust Channel Mapping
        mapping = {}
        for ch in chunk.ch_names:
            if ch in CH_ESG_BASE:
                mapping[ch] = 'eeg'
            elif ch == CH_ERB:
                mapping[ch] = 'emg'
            elif ch == CH_AXI:
                mapping[ch] = 'eog'
            else:
                mapping[ch] = 'misc'
        
        chunk.set_channel_types(mapping, verbose=False)

        # Adaptive Thresholds
        t_esg = get_thr(chunk, 'eeg', 100)
        t_erb = get_thr(chunk, 'emg', 250)
        t_axi = get_thr(chunk, 'eog', 400)

        # Rejection
        chunk.drop_bad(reject={
            'eeg': t_esg * 1e-6, 
            'emg': t_erb * 1e-6, 
            'eog': t_axi * 1e-6
        }, verbose=False)
        
        # Save to disk
        chunk_path = TEMP_DIR / f"chunk_{i}_clean-epo.fif"
        chunk.save(chunk_path, overwrite=True, verbose=False)
        chunk_files.append(chunk_path)
        
        del chunk
        gc.collect()

    print("\n[+] All chunks cleaned and saved to disk.")
    
    # Final concatenation attempt (will only work if RAM allows)
    try:
        print("[+] Attempting final concatenation...")
        all_chunks = [mne.read_epochs(f, preload=True, verbose=False) for f in chunk_files]
        final_epochs = mne.concatenate_epochs(all_chunks)
        out_path = OUT_DIR / "Final_Clean_Giant_Epochs-epo.fif"
        final_epochs.save(out_path, overwrite=True, verbose=True)
        print(f" >> SUCCESS: Final file saved as {out_path.name}")
        
        # Optional: cleanup temporary chunks if successful
        # for f in chunk_files: os.remove(f)
        
    except Exception as e:
        print(f" !! Concatenation failed due to RAM limits, but chunks are safe in: {TEMP_DIR}")
        print(f" Error: {e}")

if __name__ == "__main__":
    survival_cleaning_robust()

if __name__ == "__main__":
    survival_cleaning_with_autosave()
